# -*- coding: utf-8 -*-
"""text-emotion-detection_(1)[1].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sNW34erLstDmZvfxiW4yJ06uMhTzoCjE
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd .read_csv('emotion_sentimen_dataset.csv', encoding='utf-8')

df.head()

stop_words = set([
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're",
    "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',
    'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its',
    'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who',
    'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was',
    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',
    'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while'
])

df.shape

import re
import string
def lemmatize_word(word):
    lem_dict = {'running': 'run', 'flies': 'fly', 'dying': 'die', 'went': 'go', 'better': 'good'}
    return lem_dict.get(word, word)

# Function to expand contractions
def expand_contractions(text):
    contractions_dict = {
        "can't": "cannot", "won't": "will not", "n't": " not",
        "'re": " are", "'s": " is", "'d": " would", "'ll": " will",
        "'t": " not", "'ve": " have", "'m": " am"
    }
    for key, value in contractions_dict.items():
        text = re.sub(r"\b" + re.escape(key) + r"\b", value, text)
    return text

def clean_text(text):
    text = expand_contractions(text)  # Expand contractions
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'@\w+', '', text)  # Remove @usernames
    text = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs
    text = text.encode('ascii', 'ignore').decode('ascii')  # Remove emojis
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    text = " ".join([lemmatize_word(word) for word in text.split() if word not in stop_words])  # Lemmatization & Stopword removal
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text


# Example usage
sample_text = "I'm happy! Can't wait to see you."
print(clean_text(sample_text))

# Apply preprocessing
df['Clean_Text'] = df['text'].astype(str).apply(clean_text)

x = df['Clean_Text']
y = df['Emotion']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.4, random_state = 42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Build Logistic Regression Model with TfidfVectorizer
pipe_lr = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1,1), max_features=500)),
    ('lr', LogisticRegression(max_iter=500, solver='liblinear'))
])

from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Ensure x_train is a Series containing text
x_train = x_train.squeeze()  # Convert DataFrame to Series if x_train has a single column

# Create the pipeline
pipe_lr = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values
    ('vectorizer', TfidfVectorizer()),                      # Convert text to numeric features
    ('logreg', LogisticRegression())                        # Train logistic regression model
])

from sklearn.metrics import accuracy_score, classification_report

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import pandas as pd

# Ensure x_train is a 1D array or pandas Series containing text data
print(f"Type of x_train: {type(x_train)}")
print(f"Shape of x_train: {x_train.shape}")
print("First few entries in x_train:")
print(x_train[:5])  # or use x_train.head() if it's a pandas Series

# If x_train is a pandas Series, convert to numpy array of strings
if isinstance(x_train, pd.Series):
    x_train = x_train.astype(str).values  # Convert to numpy array of strings

# If x_train is a numpy ndarray, ensure it's a 1D array
if isinstance(x_train, np.ndarray) and len(x_train.shape) > 1:
    x_train = x_train.flatten()  # Flatten 2D array to 1D

# Check again the shape and first few values
print(f"Shape of x_train after preprocessing: {x_train.shape}")
print("First few entries in x_train after preprocessing:")
print(x_train[:5])

# Create the pipeline with TfidfVectorizer for text data
pipe_lr = Pipeline([
    ('vectorizer', TfidfVectorizer()),  # Vectorize the text data
    ('imputer', SimpleImputer(strategy='mean')),  # Handle any missing values
    ('logreg', LogisticRegression())  # Train a Logistic Regression model
])

# Fit the entire pipeline
pipe_lr.fit(x_train, y_train)  # Fit the pipeline on training data

# Generate predictions for train and test sets
y_train_pred = pipe_lr.predict(x_train)
y_test_pred = pipe_lr.predict(x_test)

# Ensure consistent label types (convert to string)
y_train = y_train.astype(str)
y_test = y_test.astype(str)
y_train_pred = y_train_pred.astype(str)
y_test_pred = y_test_pred.astype(str)

# Calculate accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

# Output results
print(f"ðŸ”¹ Logistic Regression Train Accuracy: {train_accuracy:.4f}")
print(f"ðŸ”¹ Logistic Regression Test Accuracy:  {test_accuracy:.4f}")
print(classification_report(y_test, y_test_pred))

# Check for overfitting
if train_accuracy - test_accuracy > 0.1:
    print("Potential Overfitting Detected: Significant gap between train and test accuracy.")
else:
    print("No significant overfitting detected.")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Ensure consistent label order and type
labels = sorted(df['Emotion'].astype(str).unique())

# Convert y_test and y_test_pred to string for consistency
y_test = y_test.astype(str)
y_test_pred = y_test_pred.astype(str)

# Generate confusion matrix
cm = confusion_matrix(y_test, y_test_pred, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt

# Calculate learning curve data
train_sizes, train_scores, val_scores = learning_curve(
    estimator=pipe_lr,      # Your trained pipeline
    X=x_train,              # Training features
    y=y_train,              # Training labels
    cv=5,                   # 5-fold cross-validation
    scoring='accuracy',     # Use accuracy metric
    train_sizes=np.linspace(0.1, 1.0, 5),  # 5 points from 10% to 100% of training data
    n_jobs=-1               # Use all CPU cores
)

# Calculate mean and standard deviation of scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# Plot learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, val_mean, 'o--', color='green', label='Validation Accuracy')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')

plt.title('Learning Curve - Logistic Regression')
plt.xlabel('Number of Training Samples')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.pie(df.Emotion.value_counts(), labels = df.Emotion.value_counts().index)

!pip install ydata-profiling
from ydata_profiling import ProfileReport

profile = ProfileReport(df, title="Emotion Recognition")
profile.to_notebook_iframe()

from sklearn.svm import SVC

# Define SVM model
svm = SVC(kernel='linear', probability=True)

# Define TF-IDF + SVM pipeline
pipe_svm = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1, 1), max_features=500)),
    ('svm', svm)
])

# Train and evaluate SVM
pipe_svm.fit(x_train, y_train)
y_pred_svm = pipe_svm.predict(x_test)
acc_svm = accuracy_score(y_test, y_pred_svm)

print("\n=== SVM Results ===")
print(f"Accuracy: {acc_svm:.4f}")
print(classification_report(y_test, y_pred_svm))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Ensure all labels are strings (if not already)
y_test = y_test.astype(str)
y_pred_svm = y_pred_svm.astype(str)

# Get all unique labels in sorted order for display
labels = sorted(df['Emotion'].astype(str).unique())

# Compute and plot confusion matrix
cm = confusion_matrix(y_test, y_pred_svm, labels=labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix - SVM")
plt.show()

from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt

train_sizes, train_scores, val_scores = learning_curve(
    estimator=pipe_svm,
    X=x_train,
    y=y_train,
    cv=5,
    scoring='accuracy',
    train_sizes=np.linspace(0.1, 1.0, 5),
    n_jobs=-1
)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, val_mean, 'o--', color='green', label='Validation Accuracy')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')
plt.title('Learning Curve - SVM')
plt.xlabel('Number of Training Samples')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

from xgboost import XGBClassifier

# Define XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Define TF-IDF + XGBoost pipeline
pipe_xgb = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1, 1), max_features=500)),
    ('xgb', xgb)
])

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode the labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# split the dataset
x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.4, random_state=42)

pipe_xgb.fit(x_train, y_train)
y_pred_xgb = pipe_xgb.predict(x_test)

# Evaluate
acc_xgb = accuracy_score(y_test, y_pred_xgb)
print("\n=== XGBoost Results ===")
print(f"Accuracy: {acc_xgb:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Existing evaluation
acc_xgb = accuracy_score(y_test, y_pred_xgb)
print("\n=== XGBoost Results ===")
print(f"Accuracy: {acc_xgb:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# ðŸ“Š Confusion Matrix
cm = confusion_matrix(y_test, y_pred_xgb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(xticks_rotation=45)

from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt

# Generate learning curve data for the XGBoost pipeline
train_sizes, train_scores, val_scores = learning_curve(
    estimator=pipe_xgb,
    X=x_train,
    y=y_train,
    cv=5,
    scoring='accuracy',
    train_sizes=np.linspace(0.1, 1.0, 5),
    n_jobs=-1
)

# Calculate mean and std for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, val_mean, 'o--', color='green', label='Validation Accuracy')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')
plt.title('Learning Curve - XGBoost')
plt.xlabel('Number of Training Samples')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer

# Define NaÃ¯ve Bayes model
nb = MultinomialNB()

# Define TF-IDF + NaÃ¯ve Bayes pipeline
pipe_nb = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1, 1), max_features=500)),
    ('nb', nb)
])

# Train and evaluate Naive Bayes

pipe_nb.fit(x_train, y_train)
y_pred_nb = pipe_nb.predict(x_test)
acc_nb = accuracy_score(y_test, y_pred_nb)

print("\n=== Naive Bayes Results ===")
print(f"Accuracy: {acc_nb:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred_nb))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# OPTIONAL: If you used LabelEncoder to encode the labels
# Make sure y_test and y_pred_nb are numpy arrays of encoded integers
# Decode them to original string labels
y_test_str = le.inverse_transform(y_test)
y_pred_nb_str = le.inverse_transform(y_pred_nb)

# Ensure all labels are consistent strings
labels = le.classes_.astype(str)

# Compute the confusion matrix
cm = confusion_matrix(y_test_str, y_pred_nb_str, labels=labels)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(xticks_rotation=45)
plt.title("Confusion Matrix - Naive Bayes")
plt.grid(False)
plt.tight_layout()
plt.show()

from sklearn.model_selection import learning_curve
import numpy as np
import matplotlib.pyplot as plt

train_sizes, train_scores, val_scores = learning_curve(
    estimator=pipe_nb,
    X=x_train,
    y=y_train,
    cv=5,
    scoring='accuracy',
    train_sizes=np.linspace(0.1, 1.0, 5),
    n_jobs=-1
)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, val_mean, 'o--', color='green', label='Validation Accuracy')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')
plt.title('Learning Curve - Naive Bayes')
plt.xlabel('Number of Training Samples')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

import joblib
pipeline_file = open("text_emotion.pkl","wb")
joblib.dump(pipe_lr,pipeline_file)
pipeline_file.close()

